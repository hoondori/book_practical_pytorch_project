{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9177524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "original_dataset_dir = '/data/book_practical_pytorch_project/plant_leaf'\n",
    "classes_list = os.listdir(original_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c939ac7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Tomato___Bacterial_spot', 'Tomato___Leaf_Mold']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62782b",
   "metadata": {},
   "source": [
    "# 데이터 분할을 위한 폴더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfa32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './splitted'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "valid_dir = os.path.join(base_dir, 'valid')\n",
    "os.mkdir(valid_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for clss in classes_list:\n",
    "    os.mkdir(os.path.join(train_dir, clss))\n",
    "    os.mkdir(os.path.join(valid_dir, clss))\n",
    "    os.mkdir(os.path.join(test_dir, clss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c016ae",
   "metadata": {},
   "source": [
    "# 데이터 분할과 클래스별 데이터 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dfa301",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size for class Potato___Early_blight: 600\n",
      "Valid size for class Potato___Early_blight: 200\n",
      "Test size for class Potato___Early_blight: 200\n",
      "Train size for class Tomato___Bacterial_spot: 1276\n",
      "Valid size for class Tomato___Bacterial_spot: 425\n",
      "Test size for class Tomato___Bacterial_spot: 425\n",
      "Train size for class Tomato___Leaf_Mold: 571\n",
      "Valid size for class Tomato___Leaf_Mold: 190\n",
      "Test size for class Tomato___Leaf_Mold: 190\n",
      "Train size for class Tomato___Target_Spot: 842\n",
      "Valid size for class Tomato___Target_Spot: 280\n",
      "Test size for class Tomato___Target_Spot: 280\n",
      "Train size for class Corn___Northern_Leaf_Blight: 591\n",
      "Valid size for class Corn___Northern_Leaf_Blight: 197\n",
      "Test size for class Corn___Northern_Leaf_Blight: 197\n",
      "Train size for class Tomato___healthy: 954\n",
      "Valid size for class Tomato___healthy: 318\n",
      "Test size for class Tomato___healthy: 318\n",
      "Train size for class Tomato___Tomato_Yellow_Leaf_Curl_Virus: 3214\n",
      "Valid size for class Tomato___Tomato_Yellow_Leaf_Curl_Virus: 1071\n",
      "Test size for class Tomato___Tomato_Yellow_Leaf_Curl_Virus: 1071\n",
      "Train size for class Grape___healthy: 253\n",
      "Valid size for class Grape___healthy: 84\n",
      "Test size for class Grape___healthy: 84\n",
      "Train size for class Corn___Common_rust: 715\n",
      "Valid size for class Corn___Common_rust: 238\n",
      "Test size for class Corn___Common_rust: 238\n",
      "Train size for class Apple___Black_rot: 372\n",
      "Valid size for class Apple___Black_rot: 124\n",
      "Test size for class Apple___Black_rot: 124\n",
      "Train size for class Pepper,_bell___Bacterial_spot: 598\n",
      "Valid size for class Pepper,_bell___Bacterial_spot: 199\n",
      "Test size for class Pepper,_bell___Bacterial_spot: 199\n",
      "Train size for class Potato___Late_blight: 600\n",
      "Valid size for class Potato___Late_blight: 200\n",
      "Test size for class Potato___Late_blight: 200\n",
      "Train size for class Pepper,_bell___healthy: 886\n",
      "Valid size for class Pepper,_bell___healthy: 295\n",
      "Test size for class Pepper,_bell___healthy: 295\n",
      "Train size for class Tomato___Spider_mites Two-spotted_spider_mite: 1005\n",
      "Valid size for class Tomato___Spider_mites Two-spotted_spider_mite: 335\n",
      "Test size for class Tomato___Spider_mites Two-spotted_spider_mite: 335\n",
      "Train size for class Corn___healthy: 697\n",
      "Valid size for class Corn___healthy: 232\n",
      "Test size for class Corn___healthy: 232\n",
      "Train size for class Corn___Cercospora_leaf_spot Gray_leaf_spot: 307\n",
      "Valid size for class Corn___Cercospora_leaf_spot Gray_leaf_spot: 102\n",
      "Test size for class Corn___Cercospora_leaf_spot Gray_leaf_spot: 102\n",
      "Train size for class Potato___healthy: 91\n",
      "Valid size for class Potato___healthy: 30\n",
      "Test size for class Potato___healthy: 30\n",
      "Train size for class Apple___healthy: 987\n",
      "Valid size for class Apple___healthy: 329\n",
      "Test size for class Apple___healthy: 329\n",
      "Train size for class Cherry___Powdery_mildew: 631\n",
      "Valid size for class Cherry___Powdery_mildew: 210\n",
      "Test size for class Cherry___Powdery_mildew: 210\n",
      "Train size for class Apple___Apple_scab: 378\n",
      "Valid size for class Apple___Apple_scab: 126\n",
      "Test size for class Apple___Apple_scab: 126\n",
      "Train size for class Grape___Esca_(Black_Measles): 829\n",
      "Valid size for class Grape___Esca_(Black_Measles): 276\n",
      "Test size for class Grape___Esca_(Black_Measles): 276\n",
      "Train size for class Tomato___Tomato_mosaic_virus: 223\n",
      "Valid size for class Tomato___Tomato_mosaic_virus: 74\n",
      "Test size for class Tomato___Tomato_mosaic_virus: 74\n",
      "Train size for class Tomato___Late_blight: 1145\n",
      "Valid size for class Tomato___Late_blight: 381\n",
      "Test size for class Tomato___Late_blight: 381\n",
      "Train size for class Peach___healthy: 216\n",
      "Valid size for class Peach___healthy: 72\n",
      "Test size for class Peach___healthy: 72\n",
      "Train size for class Peach___Bacterial_spot: 1378\n",
      "Valid size for class Peach___Bacterial_spot: 459\n",
      "Test size for class Peach___Bacterial_spot: 459\n",
      "Train size for class Grape___Black_rot: 708\n",
      "Valid size for class Grape___Black_rot: 236\n",
      "Test size for class Grape___Black_rot: 236\n",
      "Train size for class Strawberry___healthy: 273\n",
      "Valid size for class Strawberry___healthy: 91\n",
      "Test size for class Strawberry___healthy: 91\n",
      "Train size for class Apple___Cedar_apple_rust: 165\n",
      "Valid size for class Apple___Cedar_apple_rust: 55\n",
      "Test size for class Apple___Cedar_apple_rust: 55\n",
      "Train size for class Tomato___Septoria_leaf_spot: 1062\n",
      "Valid size for class Tomato___Septoria_leaf_spot: 354\n",
      "Test size for class Tomato___Septoria_leaf_spot: 354\n",
      "Train size for class Cherry___healthy: 512\n",
      "Valid size for class Cherry___healthy: 170\n",
      "Test size for class Cherry___healthy: 170\n",
      "Train size for class Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 645\n",
      "Valid size for class Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 215\n",
      "Test size for class Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 215\n",
      "Train size for class Tomato___Early_blight: 600\n",
      "Valid size for class Tomato___Early_blight: 200\n",
      "Test size for class Tomato___Early_blight: 200\n",
      "Train size for class Strawberry___Leaf_scorch: 665\n",
      "Valid size for class Strawberry___Leaf_scorch: 221\n",
      "Test size for class Strawberry___Leaf_scorch: 221\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for clss in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, clss)\n",
    "    fnames = os.listdir(path)\n",
    "    \n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    valid_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "    \n",
    "    train_fnames = fnames[:train_size]\n",
    "    valid_fnames = fnames[train_size:(train_size+valid_size)]\n",
    "    test_fnames = fnames[(train_size+valid_size):(train_size+valid_size+test_size)]\n",
    "    \n",
    "    def _copy(fnames, dst_dir, dataname):\n",
    "        for fname in fnames:\n",
    "            src = os.path.join(path, fname)\n",
    "            dst = os.path.join(os.path.join(dst_dir, clss), fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "        print('{} size for class {}: {}'.format(dataname, clss, len(fnames)))\n",
    "    _copy(train_fnames, train_dir, 'Train')\n",
    "    _copy(valid_fnames, valid_dir, 'Valid')\n",
    "    _copy(test_fnames, test_dir, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62722c",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 학습을 위한 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6db0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 30\n",
    "\n",
    "# 변환 및 Dataset 정의\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
    "valid_dataset = ImageFolder(root='./splitted/valid', transform=transform_base)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208759a0",
   "metadata": {},
   "source": [
    "# 베이스라인 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca10ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32,64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        \n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model_base = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ecd4f",
   "metadata": {},
   "source": [
    "# 모델 학습/평가를 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d88c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "            y_hat = model(X)\n",
    "            test_loss = F.cross_entropy(y_hat, y, reduction='sum').item()\n",
    "            \n",
    "            pred = y_hat.max(1, keepdim=True)[1]\n",
    "            \n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        return test_loss, test_accuracy            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff1b81",
   "metadata": {},
   "source": [
    "# 모델 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cf23180",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- epoch 1 --------------\n",
      "train loss: 0.0034, acc: 82.12%\n",
      "valid loss: 0.0051, acc: 79.63%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 2 --------------\n",
      "train loss: 0.0043, acc: 83.66%\n",
      "valid loss: 0.0032, acc: 81.26%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 3 --------------\n",
      "train loss: 0.0037, acc: 86.62%\n",
      "valid loss: 0.0031, acc: 83.40%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 4 --------------\n",
      "train loss: 0.0035, acc: 88.62%\n",
      "valid loss: 0.0021, acc: 85.73%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 5 --------------\n",
      "train loss: 0.0021, acc: 89.75%\n",
      "valid loss: 0.0032, acc: 86.48%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 6 --------------\n",
      "train loss: 0.0024, acc: 90.52%\n",
      "valid loss: 0.0030, acc: 86.52%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 7 --------------\n",
      "train loss: 0.0022, acc: 91.86%\n",
      "valid loss: 0.0019, acc: 88.05%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 8 --------------\n",
      "train loss: 0.0014, acc: 93.00%\n",
      "valid loss: 0.0028, acc: 88.80%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 9 --------------\n",
      "train loss: 0.0018, acc: 94.47%\n",
      "valid loss: 0.0030, acc: 90.19%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 10 --------------\n",
      "train loss: 0.0017, acc: 94.84%\n",
      "valid loss: 0.0019, acc: 90.34%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 11 --------------\n",
      "train loss: 0.0018, acc: 91.29%\n",
      "valid loss: 0.0030, acc: 86.74%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 12 --------------\n",
      "train loss: 0.0013, acc: 94.20%\n",
      "valid loss: 0.0018, acc: 89.74%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 13 --------------\n",
      "train loss: 0.0012, acc: 95.66%\n",
      "valid loss: 0.0014, acc: 90.56%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 14 --------------\n",
      "train loss: 0.0009, acc: 95.67%\n",
      "valid loss: 0.0036, acc: 90.77%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 15 --------------\n",
      "train loss: 0.0011, acc: 96.94%\n",
      "valid loss: 0.0025, acc: 91.29%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 16 --------------\n",
      "train loss: 0.0008, acc: 96.89%\n",
      "valid loss: 0.0011, acc: 91.61%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 17 --------------\n",
      "train loss: 0.0008, acc: 97.09%\n",
      "valid loss: 0.0013, acc: 92.00%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 18 --------------\n",
      "train loss: 0.0009, acc: 96.24%\n",
      "valid loss: 0.0017, acc: 91.00%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 19 --------------\n",
      "train loss: 0.0007, acc: 97.69%\n",
      "valid loss: 0.0018, acc: 92.18%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 20 --------------\n",
      "train loss: 0.0006, acc: 97.56%\n",
      "valid loss: 0.0010, acc: 91.93%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 21 --------------\n",
      "train loss: 0.0005, acc: 97.59%\n",
      "valid loss: 0.0019, acc: 92.00%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 22 --------------\n",
      "train loss: 0.0006, acc: 97.37%\n",
      "valid loss: 0.0019, acc: 91.61%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 23 --------------\n",
      "train loss: 0.0004, acc: 97.92%\n",
      "valid loss: 0.0013, acc: 92.28%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 24 --------------\n",
      "train loss: 0.0007, acc: 97.76%\n",
      "valid loss: 0.0009, acc: 91.96%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 25 --------------\n",
      "train loss: 0.0006, acc: 98.80%\n",
      "valid loss: 0.0015, acc: 93.30%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 26 --------------\n",
      "train loss: 0.0007, acc: 98.45%\n",
      "valid loss: 0.0010, acc: 92.46%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 27 --------------\n",
      "train loss: 0.0004, acc: 98.49%\n",
      "valid loss: 0.0012, acc: 92.59%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 28 --------------\n",
      "train loss: 0.0003, acc: 98.82%\n",
      "valid loss: 0.0018, acc: 93.08%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 29 --------------\n",
      "train loss: 0.0003, acc: 99.17%\n",
      "valid loss: 0.0007, acc: 93.59%\n",
      "Completed in 0m, 15s\n",
      "----------------- epoch 30 --------------\n",
      "train loss: 0.0002, acc: 99.15%\n",
      "valid loss: 0.0015, acc: 93.13%\n",
      "Completed in 0m, 15s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(1, num_epochs +1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_loader)\n",
    "        \n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('----------------- epoch {} --------------'.format(epoch))\n",
    "        print('train loss: {:.4f}, acc: {:.2f}%'.format(train_loss, train_acc))\n",
    "        print('valid loss: {:.4f}, acc: {:.2f}%'.format(valid_loss, valid_acc))\n",
    "        print('Completed in {:.0f}m, {:.0f}s'.format(time_elapsed //60, time_elapsed % 60))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "base = train_baseline(model_base, train_loader, valid_loader, optimizer, EPOCH)\n",
    "torch.save(base, 'baseline.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f52ab1",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5b7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "     'valid': transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "         \n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './splitted'\n",
    "\n",
    "image_datasets = {x:ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'valid']}\n",
    "dataloaders = {x:DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
    "dataset_sizes = {x:len(image_datasets[x])  for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdf7b3",
   "metadata": {},
   "source": [
    "## pre-trained 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b80e90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "# fc 바꿔치기\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, len(class_names))   # to number of class\n",
    "resnet.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)  # non-freeze만 학습 가능 \n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) # 7 epoch 마다 lr을 10% 씩 감소 \n",
    "\n",
    "# 일부 layer freeze\n",
    "ct = 0 \n",
    "for child in resnet.children():   # resnet 10개층 가운데 초반 5개 freeze\n",
    "    ct += 1\n",
    "    if ct < 6:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39536f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "def train_resnet50(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    \n",
    "    for epoch in range(1, num_epochs +1):\n",
    "        print('----------------- epoch {} --------------'.format(epoch))\n",
    "        since = time.time()\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0 # 모든 데이터의 lloss 총합\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for X, y in dataloaders[phase]:\n",
    "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    y_hat = model(X)\n",
    "                    _, preds = torch.max(y_hat, 1)\n",
    "                    loss = criterion(y_hat, y)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * X.size(0) # ???\n",
    "                running_corrects += torch.sum(preds == y.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                l_r = [x['lr'] for x in optimizer_ft.param_groups]\n",
    "                print('learning rate: ', l_r)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "            print('{} loss {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Completed in {:.0f}m, {:.0f}s'.format(time_elapsed //60, time_elapsed % 60))            \n",
    "    \n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fbdb883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- epoch 1 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.5962 Acc: 0.8199\n",
      "valid loss 0.2984 Acc: 0.9036\n",
      "Completed in 0m, 17s\n",
      "----------------- epoch 2 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.2077 Acc: 0.9336\n",
      "valid loss 0.1822 Acc: 0.9427\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 3 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.1710 Acc: 0.9441\n",
      "valid loss 0.1847 Acc: 0.9412\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 4 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.1360 Acc: 0.9566\n",
      "valid loss 0.1492 Acc: 0.9503\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 5 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.1135 Acc: 0.9637\n",
      "valid loss 0.1588 Acc: 0.9476\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 6 --------------\n",
      "learning rate:  [0.001]\n",
      "train loss 0.0937 Acc: 0.9694\n",
      "valid loss 0.1319 Acc: 0.9614\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 7 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0851 Acc: 0.9709\n",
      "valid loss 0.1365 Acc: 0.9582\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 8 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0481 Acc: 0.9840\n",
      "valid loss 0.0533 Acc: 0.9841\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 9 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0298 Acc: 0.9904\n",
      "valid loss 0.0451 Acc: 0.9855\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 10 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0232 Acc: 0.9929\n",
      "valid loss 0.0404 Acc: 0.9865\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 11 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0226 Acc: 0.9928\n",
      "valid loss 0.0396 Acc: 0.9879\n",
      "Completed in 0m, 17s\n",
      "----------------- epoch 12 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0183 Acc: 0.9946\n",
      "valid loss 0.0367 Acc: 0.9880\n",
      "Completed in 0m, 17s\n",
      "----------------- epoch 13 --------------\n",
      "learning rate:  [0.0001]\n",
      "train loss 0.0152 Acc: 0.9951\n",
      "valid loss 0.0404 Acc: 0.9870\n",
      "Completed in 0m, 17s\n",
      "----------------- epoch 14 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0161 Acc: 0.9948\n",
      "valid loss 0.0405 Acc: 0.9877\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 15 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0145 Acc: 0.9955\n",
      "valid loss 0.0357 Acc: 0.9887\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 16 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0130 Acc: 0.9960\n",
      "valid loss 0.0357 Acc: 0.9882\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 17 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0112 Acc: 0.9967\n",
      "valid loss 0.0362 Acc: 0.9891\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 18 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0124 Acc: 0.9957\n",
      "valid loss 0.0334 Acc: 0.9894\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 19 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0123 Acc: 0.9965\n",
      "valid loss 0.0356 Acc: 0.9886\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 20 --------------\n",
      "learning rate:  [1e-05]\n",
      "train loss 0.0128 Acc: 0.9961\n",
      "valid loss 0.0343 Acc: 0.9901\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 21 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0121 Acc: 0.9961\n",
      "valid loss 0.0365 Acc: 0.9890\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 22 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0116 Acc: 0.9962\n",
      "valid loss 0.0325 Acc: 0.9907\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 23 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0111 Acc: 0.9964\n",
      "valid loss 0.0301 Acc: 0.9902\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 24 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0119 Acc: 0.9959\n",
      "valid loss 0.0387 Acc: 0.9887\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 25 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0112 Acc: 0.9962\n",
      "valid loss 0.0340 Acc: 0.9899\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 26 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0108 Acc: 0.9970\n",
      "valid loss 0.0336 Acc: 0.9889\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 27 --------------\n",
      "learning rate:  [1.0000000000000002e-06]\n",
      "train loss 0.0113 Acc: 0.9963\n",
      "valid loss 0.0333 Acc: 0.9892\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 28 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train loss 0.0123 Acc: 0.9958\n",
      "valid loss 0.0359 Acc: 0.9879\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 29 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train loss 0.0116 Acc: 0.9964\n",
      "valid loss 0.0343 Acc: 0.9871\n",
      "Completed in 0m, 16s\n",
      "----------------- epoch 30 --------------\n",
      "learning rate:  [1.0000000000000002e-07]\n",
      "train loss 0.0107 Acc: 0.9968\n",
      "valid loss 0.0372 Acc: 0.9887\n",
      "Completed in 0m, 16s\n",
      "Best val Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "base = train_resnet50(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)\n",
    "torch.save(base, 'resnet50.pt')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4e6cb",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd46574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델용 전처리\n",
    "transform_base = transforms.Compose([transforms.Resize(([64,64])), transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted/test', transform=transform_base)\n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# transfer learning 모델용 전처리\n",
    "transform_resnet = transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "         \n",
    "    ])\n",
    "\n",
    "test_resnet = ImageFolder(root='./splitted/test', transform=transform_resnet)\n",
    "test_loader_resnet = torch.utils.data.DataLoader(test_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc85b0",
   "metadata": {},
   "source": [
    "## 베이스라인 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaad7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc = 93.8666\n"
     ]
    }
   ],
   "source": [
    "baseline = torch.load('baseline.pt')\n",
    "baseline.eval()\n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "print('baseline test acc = {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964e469",
   "metadata": {},
   "source": [
    "## Transfer learning 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d497ee64",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resnet50.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2acac9c9f96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresnet50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet50.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_resnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resnet test acc = {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoondori/anaconda3/envs/ai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoondori/anaconda3/envs/ai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoondori/anaconda3/envs/ai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resnet50.pt'"
     ]
    }
   ],
   "source": [
    "resnet50 = torch.load('resnet50.pt')\n",
    "resnet.eval()\n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resnet)\n",
    "print('Resnet test acc = {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eca80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai2)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
